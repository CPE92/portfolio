{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa62faf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T14:16:09.075404Z",
     "iopub.status.busy": "2023-11-07T14:16:09.074732Z",
     "iopub.status.idle": "2023-11-07T14:16:09.523172Z",
     "shell.execute_reply": "2023-11-07T14:16:09.522045Z"
    },
    "papermill": {
     "duration": 0.458297,
     "end_time": "2023-11-07T14:16:09.526773",
     "exception": false,
     "start_time": "2023-11-07T14:16:09.068476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data preprocessing: Creating tiles from microscopy images\n",
    "### Kaggle Competition: UBC Ovarian Cancer Subtype Classification and Outlier Detection (UBC-OCEAN)\n",
    "\n",
    "This notebook creates tiles from raw histopathology images fromt the Kaggle Ovarian Cancer Subtype Classification competition dataset. \n",
    "It should ideally be run on a Kaggle kernel, since downloading the whole training data locally (~700 GB) is cumbersome.\n",
    "\n",
    "Problem: \n",
    "1) Images are too large to be fed into CNNs (~100k x 100k pixels, up to 6 GB)-\n",
    "2) Only a small part of the images contains information about the cancer subtype\n",
    "3) Processing all training images (~700 GB) takes several hours\n",
    "\n",
    "Solution: \n",
    "1) Create tiles of size 512 x 512 pixels\n",
    "2) use filters to extract patches with maximum information about cancer subtype\n",
    "3) Use multiprocessing to extract tiles in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c59a932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T14:16:09.571163Z",
     "iopub.status.busy": "2023-11-07T14:16:09.570728Z",
     "iopub.status.idle": "2023-11-07T14:16:09.586152Z",
     "shell.execute_reply": "2023-11-07T14:16:09.585029Z"
    },
    "papermill": {
     "duration": 0.0252,
     "end_time": "2023-11-07T14:16:09.588883",
     "exception": false,
     "start_time": "2023-11-07T14:16:09.563683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_tiles(image_name,\n",
    "                        directory_images = '/kaggle/input/UBC-OCEAN/train_images/',\n",
    "                        directiory_tiles = '/kaggle/working/tiles',\n",
    "                        patch_size=2048, \n",
    "                        scale = 0.25.,\n",
    "                        num_tiles=100,\n",
    "                        threshold_black_pixel=10*3,\n",
    "                        threshold_black_background_ratio=0.1, \n",
    "                        threshold_variability=0.15):\n",
    "    \n",
    "    \"\"\"\n",
    "        input: image_name\n",
    "        return: file_paths of tiles created\n",
    "        \n",
    "        Arguments:\n",
    "        directory_images: Directory of raw images\n",
    "        directiory_tiles: Directory, into which tiles are saved\n",
    "        patch_size: Tile size in units of the original image\n",
    "        scale: scale factor for the tiles; the final tile will have dimension (patch_size*scale x patch_size*scale)\n",
    "        num_tiles: maximum number of tiles that should be extracted from image\n",
    "        threshold_black_pixel: Threshold of bw image, beneath which pixel is considered black\n",
    "        threshold_black_background_ratio: Tile is discarded, when ratio of black pixels is larger than this value\n",
    "        threshold_variability: Tile is discarded, when variability (std/mean) is smaller than this values\n",
    "    \"\"\"\n",
    "    \n",
    "    file_paths = []\n",
    "    \n",
    "    image_path = os.path.join(directory_images, image_name)\n",
    "    with Image.open(image_path) as image:\n",
    "        image_id = int(image_path.split('/')[-1][:-4])\n",
    "\n",
    "        n_patches_h = int(np.floor(image.width/patch_size))\n",
    "        n_patches_v = int(np.floor(image.height/patch_size))\n",
    "\n",
    "        grid_h = np.arange(n_patches_h)\n",
    "        random.shuffle(grid_h)\n",
    "\n",
    "        grid_v = np.arange(n_patches_v)\n",
    "        random.shuffle(grid_v)\n",
    "\n",
    "        counter = 0\n",
    "        for i in grid_h:\n",
    "            if counter == num_tiles:\n",
    "                break\n",
    "            for j in grid_v:\n",
    "                idx = i*n_patches_h+j\n",
    "                patch = image.crop((i * patch_size, j * patch_size,\n",
    "                                   i * patch_size+patch_size, j * patch_size+patch_size,))\n",
    "\n",
    "                bw = np.sum(patch, axis=2) # black and white image\n",
    "                num_black_pixels = np.sum(bw<threshold_black_pixel) # threshold \n",
    "\n",
    "                if np.mean(bw)>0:\n",
    "                    variability = (np.std(bw)/np.mean(bw))\n",
    "                else:\n",
    "                    variability = 0\n",
    "\n",
    "                # if Background is more than 10% of image, discard\n",
    "                condition1 = num_black_pixels < threshold_black_background_ratio*patch_size**2\n",
    "\n",
    "                # if variability is smaller than 0.1, discard\n",
    "                condition2 =  variability > threshold_variability\n",
    "\n",
    "                if (condition1 and condition2):\n",
    "                    if scale<1:\n",
    "                        patch = patch.resize((int(scale*patch_size), int(scale*patch_size)))\n",
    "                    fname = f'{image_id}_{counter}.png'\n",
    "                    file_path = os.path.join(directiory_tiles, fname)\n",
    "                    patch.save(file_path)\n",
    "                    file_paths.append(file_path)\n",
    "                    counter += 1\n",
    "                    if counter == num_tiles:\n",
    "                        break\n",
    "    if counter==0:\n",
    "        print('no patch found')\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4037647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T14:16:09.600909Z",
     "iopub.status.busy": "2023-11-07T14:16:09.600468Z",
     "iopub.status.idle": "2023-11-07T14:16:09.779958Z",
     "shell.execute_reply": "2023-11-07T14:16:09.778728Z"
    },
    "papermill": {
     "duration": 0.188392,
     "end_time": "2023-11-07T14:16:09.782617",
     "exception": false,
     "start_time": "2023-11-07T14:16:09.594225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get sizes of input images\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m im_paths \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/UBC-OCEAN/train_images/*.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m im_sizes \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(im_path) \u001b[38;5;28;01mfor\u001b[39;00m im_path \u001b[38;5;129;01min\u001b[39;00m im_paths]\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaths\u001b[39m\u001b[38;5;124m'\u001b[39m:im_paths, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msizes\u001b[39m\u001b[38;5;124m'\u001b[39m:im_sizes})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "# get sizes of input images\n",
    "im_paths = glob.glob('/kaggle/input/UBC-OCEAN/train_images/*.png')\n",
    "im_sizes = [os.path.getsize(im_path) for im_path in im_paths]\n",
    "df = pd.DataFrame({'paths':im_paths, 'sizes':im_sizes})\n",
    "df = df.sort_values('sizes', ascending=True).reset_index()\n",
    "file_paths = df['paths']\n",
    "df.head()\n",
    "sizes = [os.path.getsize(f) for f in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ca265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches of similarly sized images for multiprocessing \n",
    "start = 0\n",
    "max_size = 3e9\n",
    "\n",
    "start_list = [0]\n",
    "num_files_list = []\n",
    "while start<len(sizes):\n",
    "    num_files = np.sum([(np.cumsum(sizes[start:])<max_size)])\n",
    "    if num_files > 0:\n",
    "        num_files_list.append(num_files)\n",
    "    else:\n",
    "        num_files_list.append(1)\n",
    "    start += num_files_list[-1]\n",
    "    start_list.append(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d127a4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T14:16:09.794298Z",
     "iopub.status.busy": "2023-11-07T14:16:09.793912Z",
     "iopub.status.idle": "2023-11-07T14:16:09.800826Z",
     "shell.execute_reply": "2023-11-07T14:16:09.799451Z"
    },
    "papermill": {
     "duration": 0.01591,
     "end_time": "2023-11-07T14:16:09.803535",
     "exception": false,
     "start_time": "2023-11-07T14:16:09.787625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create task for multiprocessing\n",
    "def task(i):\n",
    "    extract_tiles_PIL(file_paths[i],\n",
    "                        directory_images = '/kaggle/input/UBC-OCEAN/train_images/',\n",
    "                        directiory_tiles = '/kaggle/working/tiles',\n",
    "                        patch_size=1200, \n",
    "                        num_tiles=100,\n",
    "                        scale=0.25,\n",
    "                        threshold_black_pixel=10*3,\n",
    "                        threshold_black_background_ratio=0.1, \n",
    "                        threshold_variability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086dcd91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T14:16:09.880978Z",
     "iopub.status.busy": "2023-11-07T14:16:09.879982Z",
     "iopub.status.idle": "2023-11-07T14:16:09.886054Z",
     "shell.execute_reply": "2023-11-07T14:16:09.885134Z"
    },
    "papermill": {
     "duration": 0.015222,
     "end_time": "2023-11-07T14:16:09.888658",
     "exception": false,
     "start_time": "2023-11-07T14:16:09.873436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.mkdir('/kaggle/working/tiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80b7b852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T14:16:09.900853Z",
     "iopub.status.busy": "2023-11-07T14:16:09.900414Z",
     "iopub.status.idle": "2023-11-07T17:02:19.815561Z",
     "shell.execute_reply": "2023-11-07T17:02:19.807885Z"
    },
    "papermill": {
     "duration": 9969.936159,
     "end_time": "2023-11-07T17:02:19.829731",
     "exception": false,
     "start_time": "2023-11-07T14:16:09.893572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 35\n",
      "Batch 0/80: Multi\n",
      "35 files processed (2.755583074 GB)\n",
      "This took 2.739610687891642 min\n",
      "\n",
      "35 8\n",
      "Batch 1/80: Multi\n",
      "43 files processed (5.692699395 GB)\n",
      "This took 5.322237706184387 min\n",
      "\n",
      "43 7\n",
      "Batch 2/80: Multi\n",
      "50 files processed (8.638527512 GB)\n",
      "This took 8.044220793247224 min\n",
      "\n",
      "50 6\n",
      "Batch 3/80: Multi\n",
      "56 files processed (11.447639553 GB)\n",
      "This took 10.28765914440155 min\n",
      "\n",
      "56 5\n",
      "Batch 4/80: Multi\n",
      "61 files processed (13.956695882 GB)\n",
      "This took 12.417055316766103 min\n",
      "\n",
      "61 5\n",
      "Batch 5/80: Multi\n",
      "66 files processed (16.57341403 GB)\n",
      "This took 14.461115423838297 min\n",
      "\n",
      "66 5\n",
      "Batch 6/80: Multi\n",
      "71 files processed (19.297620169 GB)\n",
      "This took 16.672872189680735 min\n",
      "\n",
      "71 5\n",
      "Batch 7/80: Multi\n",
      "76 files processed (22.157630479 GB)\n",
      "This took 18.81913568576177 min\n",
      "\n",
      "76 5\n",
      "Batch 8/80: Multi\n",
      "81 files processed (25.09045332 GB)\n",
      "This took 21.20768335660299 min\n",
      "\n",
      "81 4\n",
      "Batch 9/80: Multi\n",
      "85 files processed (27.524259917 GB)\n",
      "This took 22.476172542572023 min\n",
      "\n",
      "85 4\n",
      "Batch 10/80: Multi\n",
      "89 files processed (30.001061311 GB)\n",
      "This took 24.697705471515654 min\n",
      "\n",
      "89 4\n",
      "Batch 11/80: Multi\n",
      "93 files processed (32.536573563 GB)\n",
      "This took 26.625993633270262 min\n",
      "\n",
      "93 4\n",
      "Batch 12/80: Multi\n",
      "97 files processed (35.106509721 GB)\n",
      "This took 28.169607639312744 min\n",
      "\n",
      "97 4\n",
      "Batch 13/80: Multi\n",
      "101 files processed (37.774926011 GB)\n",
      "This took 30.076932756106057 min\n",
      "\n",
      "101 4\n",
      "Batch 14/80: Multi\n",
      "105 files processed (40.472317576 GB)\n",
      "This took 32.4413583834966 min\n",
      "\n",
      "105 4\n",
      "Batch 15/80: Multi\n",
      "109 files processed (43.213779691 GB)\n",
      "This took 34.687081694602966 min\n",
      "\n",
      "109 4\n",
      "Batch 16/80: Multi\n",
      "113 files processed (46.034795152 GB)\n",
      "This took 37.2009481549263 min\n",
      "\n",
      "113 4\n",
      "Batch 17/80: Multi\n",
      "117 files processed (48.931462933 GB)\n",
      "This took 39.56514063278834 min\n",
      "\n",
      "117 4\n",
      "Batch 18/80: Multi\n",
      "121 files processed (51.886736222 GB)\n",
      "This took 41.63398808638255 min\n",
      "\n",
      "121 4\n",
      "Batch 19/80: Multi\n",
      "125 files processed (54.877507487 GB)\n",
      "This took 43.908044425646466 min\n",
      "\n",
      "125 3\n",
      "Batch 20/80: Multi\n",
      "128 files processed (57.173984758 GB)\n",
      "This took 45.55594693024953 min\n",
      "\n",
      "128 3\n",
      "Batch 21/80: Multi\n",
      "131 files processed (59.476918554 GB)\n",
      "This took 47.09588831265767 min\n",
      "\n",
      "131 3\n",
      "Batch 22/80: Multi\n",
      "134 files processed (61.78993307 GB)\n",
      "This took 48.596098097165424 min\n",
      "\n",
      "134 3\n",
      "Batch 23/80: Multi\n",
      "137 files processed (64.118867896 GB)\n",
      "This took 50.129563053449 min\n",
      "\n",
      "137 3\n",
      "Batch 24/80: Multi\n",
      "140 files processed (66.495238619 GB)\n",
      "This took 51.51017551422119 min\n",
      "\n",
      "140 3\n",
      "Batch 25/80: Multi\n",
      "143 files processed (68.918109414 GB)\n",
      "This took 53.35830345948537 min\n",
      "\n",
      "143 3\n",
      "Batch 26/80: Multi\n",
      "146 files processed (71.357605143 GB)\n",
      "This took 55.35369839668274 min\n",
      "\n",
      "146 3\n",
      "Batch 27/80: Multi\n",
      "149 files processed (73.817428907 GB)\n",
      "This took 57.705059385299684 min\n",
      "\n",
      "149 3\n",
      "Batch 28/80: Multi\n",
      "152 files processed (76.343089887 GB)\n",
      "This took 59.1997918287913 min\n",
      "\n",
      "152 3\n",
      "Batch 29/80: Multi\n",
      "155 files processed (78.900829313 GB)\n",
      "This took 61.14936266740163 min\n",
      "\n",
      "155 3\n",
      "Batch 30/80: Multi\n",
      "158 files processed (81.487623575 GB)\n",
      "This took 63.24560021956761 min\n",
      "\n",
      "158 3\n",
      "Batch 31/80: Multi\n",
      "161 files processed (84.083175749 GB)\n",
      "This took 64.71353390614192 min\n",
      "\n",
      "161 3\n",
      "Batch 32/80: Multi\n",
      "164 files processed (86.72885486 GB)\n",
      "This took 67.64273972113928 min\n",
      "\n",
      "164 3\n",
      "Batch 33/80: Multi\n",
      "167 files processed (89.422767678 GB)\n",
      "This took 69.52191488742828 min\n",
      "\n",
      "167 3\n",
      "Batch 34/80: Multi\n",
      "170 files processed (92.160323407 GB)\n",
      "This took 71.37482570807138 min\n",
      "\n",
      "170 3\n",
      "Batch 35/80: Multi\n",
      "173 files processed (94.94554743 GB)\n",
      "This took 73.86967050631841 min\n",
      "\n",
      "173 3\n",
      "Batch 36/80: Multi\n",
      "176 files processed (97.753645119 GB)\n",
      "This took 75.61989543835323 min\n",
      "\n",
      "176 3\n",
      "Batch 37/80: Multi\n",
      "179 files processed (100.576245367 GB)\n",
      "This took 76.79283217191696 min\n",
      "\n",
      "179 3\n",
      "Batch 38/80: Multi\n",
      "182 files processed (103.463652113 GB)\n",
      "This took 79.43310528596243 min\n",
      "\n",
      "182 3\n",
      "Batch 39/80: Multi\n",
      "185 files processed (106.407832829 GB)\n",
      "This took 81.50744041204453 min\n",
      "\n",
      "185 2\n",
      "Batch 40/80: Multi\n",
      "187 files processed (108.433005484 GB)\n",
      "This took 83.80132697820663 min\n",
      "\n",
      "187 2\n",
      "Batch 41/80: Multi\n",
      "189 files processed (110.478842541 GB)\n",
      "This took 85.26214549938838 min\n",
      "\n",
      "189 2\n",
      "Batch 42/80: Multi\n",
      "191 files processed (112.538357784 GB)\n",
      "This took 87.02106289466222 min\n",
      "\n",
      "191 2\n",
      "Batch 43/80: Multi\n",
      "193 files processed (114.610880496 GB)\n",
      "This took 89.26664383014044 min\n",
      "\n",
      "193 2\n",
      "Batch 44/80: Multi\n",
      "195 files processed (116.695831781 GB)\n",
      "This took 91.6244012594223 min\n",
      "\n",
      "195 2\n",
      "Batch 45/80: Multi\n",
      "197 files processed (118.822740693 GB)\n",
      "This took 94.0226911743482 min\n",
      "\n",
      "197 2\n",
      "Batch 46/80: Multi\n",
      "199 files processed (120.983018545 GB)\n",
      "This took 96.61106166044871 min\n",
      "\n",
      "199 2\n",
      "Batch 47/80: Multi\n",
      "201 files processed (123.1554167 GB)\n",
      "This took 98.72946032683055 min\n",
      "\n",
      "201 2\n",
      "Batch 48/80: Multi\n",
      "203 files processed (125.343111146 GB)\n",
      "This took 100.82488415241241 min\n",
      "\n",
      "203 2\n",
      "Batch 49/80: Multi\n",
      "205 files processed (127.538336507 GB)\n",
      "This took 102.81303953727087 min\n",
      "\n",
      "205 2\n",
      "Batch 50/80: Multi\n",
      "207 files processed (129.738685637 GB)\n",
      "This took 105.06784334977468 min\n",
      "\n",
      "207 2\n",
      "Batch 51/80: Multi\n",
      "209 files processed (131.957845126 GB)\n",
      "This took 107.0274897813797 min\n",
      "\n",
      "209 2\n",
      "Batch 52/80: Multi\n",
      "211 files processed (134.192738639 GB)\n",
      "This took 108.5502771337827 min\n",
      "\n",
      "211 2\n",
      "Batch 53/80: Multi\n",
      "213 files processed (136.442582336 GB)\n",
      "This took 110.88736471335093 min\n",
      "\n",
      "213 2\n",
      "Batch 54/80: Multi\n",
      "215 files processed (138.718546938 GB)\n",
      "This took 112.60512139797211 min\n",
      "\n",
      "215 2\n",
      "Batch 55/80: Multi\n",
      "217 files processed (140.998875525 GB)\n",
      "This took 115.38148134549459 min\n",
      "\n",
      "217 2\n",
      "Batch 56/80: Multi\n",
      "219 files processed (143.288453105 GB)\n",
      "This took 118.75770372152328 min\n",
      "\n",
      "219 2\n",
      "Batch 57/80: Multi\n",
      "221 files processed (145.593758916 GB)\n",
      "This took 121.33489563465119 min\n",
      "\n",
      "221 2\n",
      "Batch 58/80: Multi\n",
      "223 files processed (147.910719324 GB)\n",
      "This took 123.99831883907318 min\n",
      "\n",
      "223 2\n",
      "Batch 59/80: Multi\n",
      "225 files processed (150.23969812 GB)\n",
      "This took 125.94597055514653 min\n",
      "\n",
      "225 2\n",
      "Batch 60/80: Multi\n",
      "227 files processed (152.570502259 GB)\n",
      "This took 127.22804241577784 min\n",
      "\n",
      "227 2\n",
      "Batch 61/80: Multi\n",
      "229 files processed (154.909076907 GB)\n",
      "This took 129.49587035973866 min\n",
      "\n",
      "229 2\n",
      "Batch 62/80: Multi\n",
      "231 files processed (157.260110998 GB)\n",
      "This took 131.68533005317053 min\n",
      "\n",
      "231 2\n",
      "Batch 63/80: Multi\n",
      "233 files processed (159.624009361 GB)\n",
      "This took 134.29085050026575 min\n",
      "\n",
      "233 2\n",
      "Batch 64/80: Multi\n",
      "235 files processed (161.994146086 GB)\n",
      "This took 136.21651285092037 min\n",
      "\n",
      "235 2\n",
      "Batch 65/80: Multi\n",
      "237 files processed (164.389438157 GB)\n",
      "This took 138.75854563713074 min\n",
      "\n",
      "237 2\n",
      "Batch 66/80: Multi\n",
      "239 files processed (166.808563429 GB)\n",
      "This took 140.61290309429168 min\n",
      "\n",
      "239 2\n",
      "Batch 67/80: Multi\n",
      "241 files processed (169.230898806 GB)\n",
      "This took 142.41659698486328 min\n",
      "\n",
      "241 2\n",
      "Batch 68/80: Multi\n",
      "243 files processed (171.682840941 GB)\n",
      "This took 144.5000615199407 min\n",
      "\n",
      "243 2\n",
      "Batch 69/80: Multi\n",
      "245 files processed (174.153237627 GB)\n",
      "This took 146.34121670722962 min\n",
      "\n",
      "245 2\n",
      "Batch 70/80: Multi\n",
      "247 files processed (176.635115006 GB)\n",
      "This took 148.42739037672678 min\n",
      "\n",
      "247 2\n",
      "Batch 71/80: Multi\n",
      "249 files processed (179.123477558 GB)\n",
      "This took 150.48852650721867 min\n",
      "\n",
      "249 2\n",
      "Batch 72/80: Multi\n",
      "251 files processed (181.630059301 GB)\n",
      "This took 152.25821573336918 min\n",
      "\n",
      "251 2\n",
      "Batch 73/80: Multi\n",
      "253 files processed (184.138663955 GB)\n",
      "This took 154.02736761569977 min\n",
      "\n",
      "253 2\n",
      "Batch 74/80: Multi\n",
      "255 files processed (186.66664468 GB)\n",
      "This took 155.60172695318857 min\n",
      "\n",
      "255 2\n",
      "Batch 75/80: Multi\n",
      "257 files processed (189.21164007 GB)\n",
      "This took 157.4177958528201 min\n",
      "\n",
      "257 2\n",
      "Batch 76/80: Multi\n",
      "259 files processed (191.765970269 GB)\n",
      "This took 160.07706705729166 min\n",
      "\n",
      "259 2\n",
      "Batch 77/80: Multi\n",
      "261 files processed (194.339033528 GB)\n",
      "This took 162.17028471628825 min\n",
      "\n",
      "261 2\n",
      "Batch 78/80: Multi\n",
      "263 files processed (196.928316551 GB)\n",
      "This took 164.444620068868 min\n",
      "\n",
      "263 2\n",
      "Batch 79/80: Multi\n",
      "265 files processed (199.538019258 GB)\n",
      "This took 166.16487490733465 min\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "# Process images\n",
    "t0 = time.time()\n",
    "\n",
    "for i, start in enumerate(start_list[:80]):\n",
    "    print()\n",
    "    num_files = start_list[i+1]-start\n",
    "    print(start, num_files)\n",
    "    if num_files >1:\n",
    "        print(f'Batch {i}/80: Multi')\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = []\n",
    "            for i in range(start, start+num_files):\n",
    "                futures.append(executor.submit(task, i))\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "            \n",
    "    else:\n",
    "        print('Single')\n",
    "        task(start)\n",
    "    print(f'{start+num_files} files processed ({np.sum(sizes[:start+num_files])/1e9} GB)')\n",
    "    print(f'This took {(time.time()-t0)/60} min')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9977.356183,
   "end_time": "2023-11-07T17:02:22.524900",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-07T14:16:05.168717",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
